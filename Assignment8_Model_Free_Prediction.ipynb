{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "* Previous lecture: Planning by dynamic programming -> solve a known MDP\n",
    "* This lecture: Model-free prediction -> estimate the value function of unknown MDP\n",
    "* Next lecture: Model-free control -> optimise the value function of an unknown MDP\n",
    "\n",
    "## 2. Monte-Carlo Learning\n",
    "* MC methods learn directly from episodes of experience and the episode shoulb be complete here. Thus, the MDP should ve episodic, which means all episodes must terminate.\n",
    "* MC is model-free: no knowledge of MDP transitions / rewards.\n",
    "* MC uses the simplest idea: value = mean return.\n",
    "\n",
    "### 2.1. Monte-Carlo Policy Evaluation\n",
    "* Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$.\n",
    "$$S_1, A_1, R_2, ..., S_k ~ \\pi$$\n",
    "* Monte-Carlo policy evaluation uses empirical mean return instead of expected return.\n",
    "\n",
    "#### 2.1.1. First-Visit Monte-Carlo Policy Evaluation\n",
    "* To evaluate state $s$\n",
    "* Consider the first time-step $t$ that state $s$ is visited in an episode\n",
    "* Increment counter $N(s) \\leftarrow N(s)+1$\n",
    "* Increment toal return $S(s) \\leftarrow S(s)+G_t$\n",
    "* Value is estimated by mean return $V(s)=S(s)/N(s)$\n",
    "* By law of large numbers, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$\n",
    "\n",
    "#### 2.1.2. Every-Visit Monte-Carlo Policy Evaluation\n",
    "* To evaluate state $s$\n",
    "* Every time-step $t$ that state $s$ is visited in an episode\n",
    "* Increment counter $N(s) \\leftarrow N(s)+1$\n",
    "* Increment toal return $S(s) \\leftarrow S(s)+G_t$\n",
    "* Value is estimated by mean return $V(s)=S(s)/N(s)$\n",
    "* By law of large numbers, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$\n",
    "\n",
    "### 2.2. Incremental Monte-Carlo Updates\n",
    "* Update $V(s)$ incrementally after episode $S_1, A_1, R_2, ..., S_T$\n",
    "* For each state $S_t$ wirh return $G_t$\n",
    "$$N(S_t) \\leftarrow N(S_t)+1$$\n",
    "$$V(S_t) \\leftarrow V(S_t)+\\frac{1}{N(S_t)}(G_t-V(S_t))$$\n",
    "* In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.\n",
    "$$V(S_t) \\leftarrow V(S_t)+ \\alpha (G_t-V(S_t))$$\n",
    "\n",
    "## 3. Temporal-Difference Learning\n",
    "* TD methods learn directly from episodes of experience, but here the episodes are incomplete.\n",
    "* TD is model-free: no knowledge of MDP transitions / rewards.\n",
    "* TD updates a guess towards a guess.\n",
    "\n",
    "### 2.1. MC and TD\n",
    "* Goal: learn $v_\\pi$ online from experience under policy $\\pi$\n",
    "* Incremental every-visit Monte-Carlo\n",
    "* Update value $V(S_t)$ toward actual return $G_t$\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t-V(S_{t+1}))$$\n",
    "* Simplest temporal-difference learning algorithm: TD(0)\n",
    "* Update value $V(S_t)$ toward estimated return $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1}+\\gamma V(S_{t+1})-V(S_t))$$\n",
    "* $R_{t+1}+\\gamma V(S_{t+1})$ is called the TD target\n",
    "* $\\delta_t = R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ is called the TD error\n",
    "\n",
    "### 2.2 Advantages and Disadvantages of MC vs. TD\n",
    "* TD can learn before knowing the final outcome\n",
    "* TD can learn online after every step\n",
    "* MC must wait until end of episode before return is known\n",
    "* TD can learn without the final outcome TD can learn from incomplete sequences\n",
    "* MC can only learn from complete sequences\n",
    "* TD works in continuing (non-terminating) environments\n",
    "* MC only works for episodic (terminating) environments\n",
    "\n",
    "### 2.3 Bias / Variance Trade-Off\n",
    "* Return $G_t=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{T-1}R_T$ is unbiased estimate of $v_\\pi (S_t)$\n",
    "* True TD target $R_{t+1}+\\gamma v_\\pi (S_{t+1})$ is unbiased estimate of $v_\\pi (S_t)$\n",
    "* TD target $R_{t+1}+\\gamma V(S_{t+1})$ is biased estimate of $v_\\pi (S_t)$\n",
    "* TD target is much lower variance than the return, because return depends on many random actions, transitions, rewards, but TD target depends on one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prove that fixed learning rate (step size alpha) for MC is equivalent to an exponentially decaying average of episode returns\n",
    "\n",
    "$$\\begin{split}\n",
    "V_{k+1}(S_{t}) & = & V_k(S_t) + \\alpha (G_{k,t}-V_k(S_{t+1})) \\\\\n",
    "& = & (1-\\alpha)V_k(S_t) + \\alpha G_{k,t} \\\\\n",
    "& = & (1-\\alpha)((1-\\alpha)V_{k-1}(S_t) + \\alpha G_{k-1,t}) + \\alpha G_{k,t} \\\\\n",
    "& = & \\alpha (1-\\alpha)^{k-1}G_{0,t} + ... + \\alpha (1-\\alpha)G_{k-1,t}+\\alpha G_{k,t}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prove that Offline Forward-View TD(Lambda) and Offline Backward View TD(Lambda) are equivalent. We covered the proof of Lambda = 1 in class. Do the proof for arbitrary Lambda (similar telescoping argument as done in class) for the case where a state appears only once in an episode.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
