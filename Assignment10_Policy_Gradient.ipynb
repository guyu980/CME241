{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Proof (with precise notation) of the Policy Gradient Theorem\n",
    "\n",
    "1. Theorem\n",
    "$$\\nabla_{\\theta}J(\\theta)=\\int_{\\mathcal S} \\rho^\\pi(s) \\int_{\\mathcal{A}} \\nabla_{\\theta}\\pi(s,a ; \\theta) Q^\\pi (s,a)dads$$\n",
    "\n",
    "2. Proof   \n",
    "We begin the proof by noting that:\n",
    "$$J(\\theta)=\\int_{\\mathcal S} p_0(s_0)V^\\pi(s_0)ds_0=\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal{A}} \\pi(s_0,a_0 ; \\theta) Q^\\pi(s_0,a_0)da_0ds_0$$\n",
    "Calculate $\\nabla_{\\theta}J(\\theta)$ by parts $\\pi(s_0,a_0;\\theta)$ and $Q^\\pi(s_0,a_0)$:\n",
    "$$\\begin{aligned} \\nabla_{\\theta} J(\\theta) &=\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal{A}} \\nabla_{\\theta} \\pi(s_0,a_0;\\theta) Q^{\\pi}(s_0, a_{0}) d a_0 d s_0 \\\\ &+\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal A} \\pi(s_0, a_0 ; \\theta) \\nabla_{\\theta} Q^\\pi(s_0, a_0) d a_0 d s_0 \\end{aligned}$$\n",
    "Now expand $Q^\\pi(s_0, a_0)$ by Bellman and note that $\\nabla_\\theta \\mathcal R_{s_0}^{a_0}=0$:\n",
    "$$\\begin{aligned} \\nabla_{\\theta} J(\\theta) &=\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal{A}} \\nabla_{\\theta} \\pi(s_0,a_0;\\theta) Q^{\\pi}(s_0, a_{0}) d a_0 d s_0 \\\\ &+\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal A} \\pi(s_0, a_0 ; \\theta) \\nabla_{\\theta}(\\int_{\\mathcal S} \\gamma \\mathcal P_{s0,s1}^{a_0}V^\\pi(s_1)ds_1) d a_0 d s_0 \\end{aligned}$$\n",
    "Then change the integeral sign:\n",
    "$$\\begin{aligned} \\nabla_{\\theta} J(\\theta) &=\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal{A}} \\nabla_{\\theta} \\pi(s_0,a_0;\\theta) Q^{\\pi}(s_0, a_{0}) d a_0 d s_0 \\\\ &+\\int_{\\mathcal S}(\\int_{\\mathcal S} \\gamma p_0(s_0) \\int_{\\mathcal A} \\pi(s_0, a_0 ; \\theta)\\mathcal P_{s0,s1}^{a_0}   d a_0 d s_0) \\nabla_{\\theta}V^\\pi(s_1)ds_1 \\end{aligned}$$\n",
    "Note that $\\int_{\\mathcal A} \\pi(s_0, a_0 ; \\theta)\\mathcal P_{s_0,s_1}^{a_0}   d a_0=p(s_0 \\rightarrow s_1,1,\\pi)$ and expend $V^\\pi(s_1)$:\n",
    "$$\\begin{aligned} \\nabla_{\\theta} J(\\theta) &=\\int_{\\mathcal S} p_0(s_0) \\int_{\\mathcal{A}} \\nabla_{\\theta} \\pi(s_0,a_0;\\theta) Q^{\\pi}(s_0, a_{0}) d a_0 d s_0 \\\\ &+\\int_{\\mathcal S}(\\int_{\\mathcal S} \\gamma p_0(s_0)p(s_0 \\rightarrow s_1,1,\\pi)ds_0)\\nabla_\\theta(\\int_{\\mathcal A} \\pi(s_1, a_1 ; \\theta)Q^\\pi(s_1,a_1)d a_1 )ds_1 \\end{aligned}$$\n",
    "Now back to when we started calculating gradient of $\\int_{\\mathcal A}\\pi Q^\\pi da$, follow the same spliting process for $\\pi Q^\\pi$, then Bellman-expanding $Q^\\pi$ and iterate. The iterative process leads to:\n",
    "$$\\nabla_{\\theta} J(\\theta)=\\sum_{t=0}^\\infty \\int_{\\mathcal S}\\int_{\\mathcal S} \\gamma^t p_0(s_0)p(s_0 \\rightarrow s_t,t,\\pi)ds_0 \\int_{\\mathcal A} \\nabla_\\theta \\pi(s_t,a_t;\\theta)Q^\\pi(s_t,a_t)da_tds_t$$\n",
    "Bring $\\sum_{t=0}^\\infty$ inside the two $\\int_{\\mathcal S}$, note that the last term is independent of $t$:\n",
    "$$\\nabla_{\\theta} J(\\theta)=\\int_{\\mathcal S}\\int_{\\mathcal S}\\sum_{t=0}^\\infty \\gamma^t p_0(s_0)p(s_0 \\rightarrow s_t,t,\\pi)ds_0 \\int_{\\mathcal A} \\nabla_\\theta \\pi(s,a;\\theta)Q^\\pi(s,a)dads$$\n",
    "Reminder that $\\rho^\\pi(s)=\\int_{\\mathcal S}\\sum_{t=0}^\\infty \\gamma^t p_0(s_0)p(s_0 \\rightarrow s_t,t,\\pi)ds_0$, so:\n",
    "$$\\nabla_{\\theta}J(\\theta)=\\int_{\\mathcal S} \\rho^\\pi(s) \\int_{\\mathcal{A}} \\nabla_{\\theta}\\pi(s,a ; \\theta) Q^\\pi (s,a)dads$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive the score function for softmax policy (for finite set of actions)\n",
    "$$\\pi(s, a ; \\theta)=\\frac{e^{\\theta^{T} \\cdot \\phi(s, a)}}{\\sum_{b} e^{\\theta^{T} \\cdot \\phi(s, b)}} \\text { for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s, a ; \\theta)=\\phi(s, a)-\\sum_{b} \\pi(s, b ; \\theta) \\cdot \\phi(s, b)=\\phi(s, a)-\\mathbb{E}_{\\pi}[\\phi(s, \\cdot)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive the score function for gaussian policy (for continuous actions)\n",
    "$$\\nabla_{\\theta} \\log \\pi(s, a ; \\theta)=\\frac{\\left(a-\\theta^{T} \\cdot \\phi(s)\\right) \\cdot \\phi(s)}{\\sigma^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Proof (with proper notation) of the Compatible Function Approximation Theorem\n",
    "1. Theorem\n",
    "If the following two conditions are satisfied:  \n",
    "(1) Critic gradient is compatible with the Actor score function:\n",
    "$$\\nabla_w Q(s,a;w)=\\nabla_\\theta log\\pi(s,a,\\theta)$$\n",
    "(2) Critic parameters w minimize the following mean-squared error:\n",
    "$$\\epsilon=\\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A}\\pi(s,a;\\theta)(Q^\\pi(s,a)-Q(s,a;w))^2dads$$\n",
    "Then the policy gradient unsing critic Q(s,a;w) is exact:\n",
    "$$\\nabla_\\theta J(\\theta)=\\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A}\\nabla_\\theta \\pi(s,a,\\theta)Q(s,a;w)dads$$  \n",
    "2. Proof  \n",
    "For $w$ that minimizes:\n",
    "$$\\begin{array}{c}{\\epsilon=\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta) (Q^{\\pi}(s, a)-Q(s, a ; w))^{2}  dads} \\\\ {\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta) \\cdot(Q^{\\pi}(s, a)-Q(s, a ; w)) \\nabla_{w} Q(s, a ; w) dads=0}\\end{array}$$\n",
    "Since $\\nabla_{w} Q(s, a ; w)=\\nabla_{\\theta} \\log \\pi(s, a ; \\theta)$, we have:\n",
    "$$\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta) (Q^{\\pi}(s, a)-Q(s, a ; w))\\nabla_{\\theta} \\log \\pi(s, a ; \\theta)dads=0$$\n",
    "Therefore:\n",
    "$$\\begin{array}{c}{\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta)Q^{\\pi}(s, a) \\nabla_{\\theta} \\log \\pi(s, a ; \\theta)  dads} \\\\ {=\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta) Q(s, a ; w) \\nabla_{\\theta} \\log \\pi(s, a ; \\theta) dads}\\end{array}$$\n",
    "But:\n",
    "$$\\nabla_{\\theta} J(\\theta)=\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta)Q^{\\pi}(s, a)\\nabla_{\\theta} \\log \\pi(s, a ; \\theta)dads$$\n",
    "So:\n",
    "$$\\begin{aligned}\\nabla_{\\theta} J(\\theta) &=\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\pi(s, a ; \\theta)Q(s, a ; w) \\nabla_{\\theta} \\log \\pi(s, a ; \\theta)dads \\\\ &=\\int_{\\mathcal{S}} \\rho^{\\pi}(s) \\int_{\\mathcal{A}} \\nabla_{\\theta} \\pi(s, a ; \\theta)Q(s, a ; w) dads \\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
