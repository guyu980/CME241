{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Value Function for MRP\n",
    "The $state \\ value \\ function \\ v(s)$ of an MRP is the expected return starting from state s. It gives the long-term value of state s.\n",
    "$$v(s)=E[G_t \\mid S_t=s]$$\n",
    "\n",
    "### 2.5 Bellman Equation for MRP\n",
    "The value function can be decomposed into 2 parts:\n",
    "* Immediate reward $R_{t+1}$.\n",
    "* Discounted value of successor state $\\gamma v(S_{t+1})$.\n",
    "$$\\begin{split}\n",
    "v(s) & = & E[G_t \\mid S_t=s] \\\\\n",
    "& = & E[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3}+... \\mid S_t=s] \\\\\n",
    "& = & E[R_{t+1}+ \\gamma G_{t+1} \\mid S_t=s] \\\\\n",
    "& = & E[R_{t+1}+ \\gamma v(S_{t+1}) \\mid S_t=s] \\\\\n",
    "& = & \\mathcal R_{s} + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}v(s^{'})\n",
    "\\end{split}$$\n",
    "Rewrite this to matrix form, we have:\n",
    "$$v=\\mathcal R + \\gamma \\mathcal P v$$\n",
    "Then we can solve the Bellman equation by:\n",
    "$$v=(I- \\gamma \\mathcal P)^{-1} \\mathcal R$$\n",
    "* The computational complexity is $(n^3)$ for n states.\n",
    "* Direct solution only possible for small MRPs.\n",
    "* Some iterative methods: dynamic programming, Monte-Carlo evaluation.\n",
    "\n",
    "## 3. Markov Decision Process\n",
    "A $Markov \\ decision \\ Process$ is a Markov reward process with decisions. Then, a MDP has 5 components:\n",
    "* A finite set of states which satisfy Markov property.\n",
    "* A finite set of actions.\n",
    "* A corresponding state transition probability matrix: (now it's a 3-dimension matrix)\n",
    "$$\\mathcal P_{ss^{'}}^a=P(S_{t+1}=s^{'} \\mid S_t=s, A_t=a)$$\n",
    "* A reward function:\n",
    "$$\\mathcal R_{s}^a=E[R_{t+1} \\mid S_t=s, A_t=a]$$\n",
    "* A discount factor.\n",
    "\n",
    "### 3.1 Policies\n",
    "A $policy \\ \\pi$ is a distribution over actions given states:\n",
    "$$\\pi(a \\mid s)=P[A_t=a \\mid S_t=s]$$\n",
    "* A policy fully defines the behaviour of an agent.\n",
    "* MDP policies depend on the current state (not the history).\n",
    "* Given an MDP $\\mathcal {M = <S, A ,P ,R}, \\gamma>$ and a policy $\\pi$.  \n",
    "* The state sequence, $S_1, S_2, ...$ is a MP $\\mathcal {<S, P^\\pi}>$.\n",
    "* The state and reward sequance, $S_1, R_2, S_2, ...$ is a MRP $\\mathcal {<S, P^\\pi ,R^\\pi}, \\gamma>$.\n",
    "* where,\n",
    "$$\\mathcal P_{s,s^{'}}^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal P_{ss^{'}}^a$$\n",
    "$$\\mathcal R_s^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal R_s^a$$\n",
    "\n",
    "### 3.2 Value Function for MDP\n",
    "There are 2 definitions for the value function of MDP based on state or action.  \n",
    "The $state-value \\ function \\ v_\\pi(s)$ of an MDP is the expected return starting from state $s$ and then following policy $\\pi$:\n",
    "$$v_\\pi(s)=E_\\pi[G_t \\mid S_t=s]$$\n",
    "The $action-value \\ function \\ q_\\pi(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and then following policy $\\pi$:\n",
    "$$q_\\pi(s,a)=E_\\pi[G_t \\mid S_t=s, A_t=a]$$\n",
    "\n",
    "### 3.3 Bellman Equation for MDP\n",
    "Both state-value and action-value function can again be decomposed intp immediate reward plus discounted value of successor state:\n",
    "$$v_\\pi(s)=E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t=s]$$\n",
    "$$q_\\pi(s,a)=E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a]$$\n",
    "* Consider one-step expectation for $v_\\pi(s)$, we have:\n",
    "$$v_\\pi(s)=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) q_\\pi(s,a)$$\n",
    "* Consider one-step expectation for $q_\\pi(s,a)$, we have:\n",
    "$$q_\\pi(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_\\pi(s^{'})$$\n",
    "* Combine these 2 equations, we have iterative expressions for $v_\\pi(s)$ and $q_\\pi(s,a)$:\n",
    "$$v_\\pi(s)=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\Big (\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_\\pi(s^{'}) \\Big)$$\n",
    "$$q_\\pi(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a \\sum_{a^{'} \\in \\mathcal A} \\pi(a^{'} \\mid s^{'}) q_\\pi(s^{'},a^{'})$$\n",
    "\n",
    "### 3.4 Optimal Value Function\n",
    "The $optimal \\ state-value \\ function \\ v_*(s)$ is the maximum state-value function over all policies:\n",
    "$$v_*(s) = \\max _\\pi v_\\pi(s)$$\n",
    "The $optimal \\ action-value \\ function \\ q_*(s,a)$ is the maximum action-value function over all policies:\n",
    "$$q_*(s,a) = \\max _\\pi q_\\pi(s,a)$$\n",
    "* The optimal value function specifies the best possible performance in the MDP.\n",
    "* An MDP is \"solved\" when we know the optimal value function.\n",
    "\n",
    "### 3.5 Optimal policy\n",
    "Define the inequality relation over policies as: $\\pi >= \\pi^{'} $ if $ v_\\pi(s) >= v_{\\pi^{'}}(s), \\forall s$.  \n",
    "Theorem: For any MDP,\n",
    "* There exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* >= \\pi, \\forall \\pi$.\n",
    "* All optimal policies achieve the optimal state-value function, $v_{\\pi_*(s)}=v_*(s)$.\n",
    "* All optimal policies achieve the optimal action-value function, $q_{\\pi_*(s,a)}=q_*(s,a)$.  \n",
    "  \n",
    "An optimal policy can be found by maximising over $q_*(s,a)$,\n",
    "$$\\pi_*(a \\mid s)=\n",
    "\\begin{cases}\n",
    "1& \\text{if $a=\\mathop{\\arg\\max}_{a \\in \\mathcal A}q_*(s,a)$}\\\\\n",
    "0& \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "* There is always a deterministic optimal policy for any MDP.\n",
    "* If we know $q_*(s,a)$, we immediately have the optimal policy.\n",
    "\n",
    "### 3.6 Bellman Optimality Equation\n",
    "* Consider the optimal policy case in the one-step Bellman equation, we have:\n",
    "$$v_*(s)=\\max_a q_*(s,a)$$\n",
    "$$q_*(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_*(s^{'})$$\n",
    "* Again, combine these 2 equations, we have iterative expressions for  $v_*(s)$ and $q_*(s,a)$ :\n",
    "$$v_*(s)=\\max_a \\Big (\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_*(s^{'}) \\Big)$$\n",
    "$$q_*(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a \\max_{a^{'}} q_*(s^{'},a^{'})$$\n",
    "* Bellman optimality equation is non-linear due to max in the equation.\n",
    "* So there is no closed form solution in general.\n",
    "* We can use iterative methods to solve it.\n",
    "\n",
    "### 3.7 Class Design of MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp import MP\n",
    "from mrp import MRP\n",
    "from policy import Policy\n",
    "from typing import Mapping, List\n",
    "from utils.generic_typevars import S, A\n",
    "from utils.utils import sum_dicts \n",
    "\n",
    "class MDP(MP):\n",
    "    def __init__(self, transitions: Mapping[S, Mapping[A, Mapping[S, float]]], \\\n",
    "                 rewards: Mapping[S, Mapping[A, Mapping[S, float]]], gamma: float) -> None:\n",
    "        self.transitions = transitions\n",
    "        self.states = self.get_all_states()\n",
    "        self.actions = self.get_all_actions()\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_all_actions(self) -> List:\n",
    "        return list(set().union(*list(self.transitions.values())))\n",
    "    \n",
    "    def get_mrp(self, pol: Policy) -> MRP:\n",
    "        transitions = {s: sum_dicts([{s1: p * v2 for s1, v2 in v[a].items()}\n",
    "                        for a, p in pol.data[s].items()])\n",
    "                        for s, v in self.transitions.items()}\n",
    "        rewards = {s: sum(p * v[a] for a, p in pol.data[s].items())\n",
    "                    for s, v in self.rewards.items()}\n",
    "        return MRP(transitions, rewards, self.gamma)\n",
    "        \n",
    "    def get_state_value_func(self, pol: Policy) -> Mapping[S, float]:\n",
    "        mrp = self.get_mrp(pol)\n",
    "        value_func = mrp.get_value_func()\n",
    "        return {mrp.states[i]: value_func[i] for i in range(len(mrp.states))}\n",
    "    \n",
    "    def get_action_value_func(self, pol: Policy) -> Mapping[S, Mapping[A, float]]:\n",
    "        value_func = self.get_state_value_func(pol)\n",
    "        return {s:  {a: r + self.gamma * sum(p * value_func[s1]\n",
    "                for s1, p in self.transitions[s][a].items())\n",
    "                for a, r in v.items()}\n",
    "                for s, v in self.rewards.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [1, 2, 3] \n",
      "\n",
      "Actions ['a', 'b', 'c'] \n",
      "\n",
      "Transition Matrix of MDP:\n",
      " {1: {'a': {1: 0.3, 2: 0.6, 3: 0.1}, 'b': {2: 0.3, 3: 0.7}, 'c': {1: 0.2, 2: 0.4, 3: 0.4}}, 2: {'a': {1: 0.3, 2: 0.6, 3: 0.1}, 'c': {1: 0.2, 2: 0.4, 3: 0.4}}, 3: {'b': {3: 1.0}}} \n",
      "\n",
      "Reward Function of MDP:\n",
      " {1: {'a': 5, 'b': 4, 'c': -6}, 2: {'a': 5, 'c': -6}, 3: {'b': 0}} \n",
      "\n",
      "Policy:\n",
      " {1: {'a': 0.4, 'b': 0.6}, 2: {'a': 0.7, 'c': 0.3}, 3: {'b': 1.0}} \n",
      "\n",
      "Transition Matrix of MRP:\n",
      " [[0.12 0.42 0.46]\n",
      " [0.27 0.54 0.19]\n",
      " [0.   0.   1.  ]] \n",
      "\n",
      "Reward Function of MRP:\n",
      " [4.4, 1.7000000000000002, 0.0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transitions = {\n",
    "    1: {\n",
    "        'a': {1: 0.3, 2: 0.6, 3: 0.1},\n",
    "        'b': {2: 0.3, 3: 0.7},\n",
    "        'c': {1: 0.2, 2: 0.4, 3: 0.4}\n",
    "    },\n",
    "    2: {\n",
    "        'a': {1: 0.3, 2: 0.6, 3: 0.1},\n",
    "        'c': {1: 0.2, 2: 0.4, 3: 0.4}\n",
    "    },\n",
    "    3: {\n",
    "        'b': {3: 1.0}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    1: {'a': 5, 'b': 4, 'c': -6},\n",
    "    2: {'a': 5, 'c': -6},\n",
    "    3: {'b': 0}\n",
    "}\n",
    "policy_data = {\n",
    "    1: {'a': 0.4, 'b': 0.6},\n",
    "    2: {'a': 0.7, 'c': 0.3},\n",
    "    3: {'b': 1.0}\n",
    "}\n",
    "mdp = MDP(transitions, rewards, 0.5)\n",
    "pol = Policy(policy_data)\n",
    "print('States:', mdp.states, '\\n')\n",
    "print('Actions', mdp.actions, '\\n')\n",
    "print('Transition Matrix of MDP:\\n', mdp.transitions, '\\n')\n",
    "print('Reward Function of MDP:\\n', mdp.rewards, '\\n')\n",
    "print('Policy:\\n', pol, '\\n')\n",
    "mrp = mdp.get_mrp(pol)\n",
    "print('Transition Matrix of MRP:\\n', mrp.transition_matrix, '\\n')\n",
    "print('Reward Function of MRP:\\n', mrp.reward_func, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
