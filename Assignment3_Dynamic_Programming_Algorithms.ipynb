{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iterative Policy Evaluation\n",
    "* Problem: evaluate a given policy $\\pi$.\n",
    "* Solution: iterative application of Bellman expectation backup. \n",
    " \n",
    "$$v_{k+1}=\\mathcal R_\\pi + \\gamma \\mathcal P_\\pi v_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import is_equal\n",
    "\n",
    "# All functions below are in the class MDP\n",
    "def iterative_policy_evaluation(self, pol: Policy) -> np.array:\n",
    "    mrp = self.get_mrp(pol)\n",
    "    v0 = np.zeros(len(self.states))\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        v1 = mrp.reward_func + self.gamma*mrp.transition_matrix.dot(v0)\n",
    "        converge = is_equal(np.linalg.norm(v1), np.linalg.norm(v0))\n",
    "        v0 = v1\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration\n",
    "* Policy evaluation: estimate $v_\\pi$ by some policy evaluation algorithm (eg. iterative)\n",
    "* policy improvement: generate $\\pi^{'}>=\\pi$ by some policy improvement algorithm (eg. greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Here we consider the deterministic policy case, \n",
    "# so the policy data will be like: {state: {action: 1.}}\n",
    "# each state only map to one action with probability of 1\n",
    "def greedy_improvement_policy(self, pol: Policy) -> Policy:\n",
    "    q = self.get_action_value_func(pol)\n",
    "    return Policy(x = {s: {max(v.items(), key=itemgetter(1))[0]: 1}\n",
    "                        for s, v in q.items()})\n",
    "    \n",
    "def policy_iteration(self, pol: Policy):\n",
    "    pol = Policy({s: {a: 1. / len(v) for a in v} for s, v in\n",
    "                    self.rewards.items()})\n",
    "    v_old = self.get_state_value_func(pol)\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        pol = self.greedy_improved_policy(pol)\n",
    "        v_new = self.iterative_policy_evaluation(pol)\n",
    "        converge = is_equal(np.linalg.norm(v_new), np.linalg.norm(v_old))\n",
    "        v_old = v_new\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Iteration\n",
    "* Problem: find optimal policy $\\pi$.\n",
    "* Solution: iterative application of Bellman optimality backup. \n",
    " \n",
    "$$v_{k+1}=\\max_{a \\in \\mathcal A} \\mathcal R^a + \\gamma \\mathcal P^av_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
